{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Word-Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using GloVe: 0.802\n",
      "Similarity between 'india' and 'indian' using GloVe: 0.865\n",
      "Similarity between 'fame' and 'famous' using GloVe: 0.589\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "glove_model = load('glove-wiki-gigaword-50')\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "\tsimilarity = glove_model.similarity(pair[0], pair[1])\n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using FastText: 0.642\n",
      "Similarity between 'india' and 'indian' using FastText: 0.708\n",
      "Similarity between 'fame' and 'famous' using FastText: 0.519\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") ## Load the pre-trained fastText model\n",
    "# Define word pairs to compute similarity for\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "\tsimilarity = fasttext_model.similarity(pair[0], pair[1])\n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using BERT: 0.930\n",
      "Similarity between 'india' and 'indian' using BERT: 0.957\n",
      "Similarity between 'fame' and 'famous' using BERT: 0.956\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "\ttokens = tokenizer(pair, return_tensors='pt')\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(**tokens)\n",
    "\t\n",
    "\t# Extract embeddings for the [CLS] token\n",
    "\tcls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "\tsimilarity = torch.nn.functional.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0)\n",
    "\t\n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using Word2Vec: 0.637\n",
      "Similarity between 'india' and 'indian' using Word2Vec: 0.697\n",
      "Similarity between 'fame' and 'famous' using Word2Vec: 0.326\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # Load Word2Vec Google News model\n",
    "\n",
    "# Define word pairs to compute similarity for\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    similarity = word2vec_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
